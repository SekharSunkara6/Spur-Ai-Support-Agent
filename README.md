# âœ¨ Spur AI Support Agent

Mini AI support agent for a live chat widget, built as part of **Spur â€“ Founding Fullâ€‘Stack Engineer Takeâ€‘Home**.  
It simulates a small eâ€‘commerce support agent that answers customer questions using a real LLM and persists conversations in **PostgreSQL (Neon)**.

***

## ğŸŒ Live URLs

- ğŸ’¬ **Chat UI (frontend)**: `https://spur-ai-support-agent.vercel.app`  
- ğŸ› ï¸ **Backend status page / API**: `https://spur-ai-support-agent.onrender.com`

Each deployment is independent but wired via environment variables and CORS.

---

## ğŸš€ 1. Quick Start (Local Setup)

### 1.1. Clone the repository

```bash
git clone https://github.com/SekharSunkara6/Spur-Ai-Support-Agent.git
cd Spur-Ai-Support-Agent
```

Repository layout:

- `backend/` â€“ Node.js + TypeScript API server  
- `frontend/` â€“ Svelte (Vite) chat UI  

### 1.2. Prerequisites

- ğŸŸ¢ Node.js 20+  
- ğŸŸ¢ npm (or pnpm/yarn)  
- ğŸŸ¢ PostgreSQL database (Neon in production; any Postgres works locally)

***

## ğŸ” 2. Environment Variables

Secrets are **not** committed. Create `.env` files from the provided examples.

### 2.1. Backend `.env`

Create `backend/.env`:

```env
# Backend server
PORT=3001

# PostgreSQL (Neon or local Postgres)
DATABASE_URL=postgres://USER:PASSWORD@HOST:PORT/DB_NAME

# LLM provider (OpenAI)
OPENAI_API_KEY=sk-...

# Where the frontend is hosted
# Local dev:
FRONTEND_URL=http://localhost:5173
# Render (production) uses:
# FRONTEND_URL=https://spur-ai-support-agent.vercel.app
```

> There is also `backend/.env.example` as a reference; keep `.env` itself out of git.

### 2.2. Frontend `.env`

Create `frontend/.env`:

```env
# Where the backend is running
# Local dev:
VITE_API_BASE_URL=http://localhost:3001

# Production (Vercel) uses:
# VITE_API_BASE_URL=https://spur-ai-support-agent.onrender.com
```

`VITE_` prefix is required so Vite exposes the variable to the browser.

***

## ğŸ—„ï¸ 3. Database Setup (Neon / Postgres)

The app expects a PostgreSQL database with two tables:

```sql
-- conversations table
CREATE TABLE IF NOT EXISTS conversations (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

-- messages table
CREATE TABLE IF NOT EXISTS messages (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  conversation_id UUID NOT NULL REFERENCES conversations(id) ON DELETE CASCADE,
  sender TEXT NOT NULL CHECK (sender IN ('user', 'ai')),
  text TEXT NOT NULL,
  timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW()
);
```

### 3.1. Using Neon

1. ğŸ†• Create a project + database in **Neon**.  
2. ğŸ”‘ Copy the connection string into `DATABASE_URL` in `backend/.env`.  
3. ğŸ“œ Run the SQL above via Neonâ€™s SQL editor or `psql`.

### 3.2. Local Postgres

```bash
createdb spur_ai_support
psql spur_ai_support < schema.sql   # schema.sql contains the SQL above
```

Then:

```env
DATABASE_URL=postgres://postgres:password@localhost:5432/spur_ai_support
```

No additional seeding is required; FAQ/domain knowledge comes from the prompt.

---

## ğŸ§‘â€ğŸ’» 4. Running Locally

### 4.1. Backend

```bash
cd backend
npm install
npm run dev   # or npm start for compiled build
```

- ğŸ”— Server: `http://localhost:3001`  
- ğŸ“Š Status page at `/` shows:
  - API health  
  - DB connectivity  
  - Button to open the chat UI  

Key endpoints:

- `POST /api/chat/message`  
- `GET  /api/chat/history/:sessionId`

### 4.2. Frontend

In another terminal:

```bash
cd frontend
npm install
npm run dev
```

- ğŸ”— Vite dev server: `http://localhost:5173`  
- Chat UI talks to the backend using `VITE_API_BASE_URL`.

***

## ğŸ“¡ 5. API Contract

### 5.1. `POST /api/chat/message`

**Request body:**

```json
{
  "message": "What is your return policy?",
  "sessionId": "optional-conversation-id"
}
```

- `message` â€“ required, nonâ€‘empty string.  
- `sessionId` â€“ optional; if missing, a new conversation is created.

**Response:**

```json
{
  "reply": "We offer 30-day returns on unused items...",
  "sessionId": "a9c4a1b5-..."
}
```

### 5.2. `GET /api/chat/history/:sessionId`

**Response:**

```json
{
  "messages": [
    { "sender": "user", "text": "...", "timestamp": "..." },
    { "sender": "ai",   "text": "...", "timestamp": "..." }
  ]
}
```

Used by the frontend on mount to restore previous chats.

***

## ğŸ’¬ 6. Frontend (Chat UI)

Built with **Svelte + Vite**.

Key features:

- ğŸ§¾ Scrollable message list with autoâ€‘scroll to the latest message.  
- ğŸ¨ Clear distinction between user and AI bubbles (alignment + color).  
- âŒ¨ï¸ Input box + send button; **Enter** sends, **Shift+Enter** adds a newline.  
- â³ Disabled send button while a request is in flight.  
- ğŸ’­ â€œAgent is typingâ€¦â€ dot animation while waiting for the LLM.  
- ğŸ•’ Perâ€‘message timestamps that stay readable in light/dark themes.  
- ğŸŒ— Theme toggle with preference saved in `localStorage`.  
- â™»ï¸ Session ID stored in `localStorage` to reload history on refresh.

---

## ğŸ§± 7. Backend Architecture Overview

The backend is a small **Node.js + TypeScript** service:

- **HTTP server / entrypoint**
  - Sets up Express, JSON parsing, CORS, and the status page.
  - Mounts `/api/chat` routes.

- **Routes**
  - `POST /api/chat/message`  
    - Validate input.  
    - Fetch conversation history from DB.  
    - Call LLM service.  
    - Persist user + AI messages.  
  - `GET /api/chat/history/:sessionId`  
    - Load messages for a given conversation.

- **Services**
  - ğŸ”® **LLM service**  
    - `generateReply(history, userMessage)` wraps the OpenAI SDK.  
    - Contains system prompt, token limits, and error handling.  
  - ğŸ’¾ **Conversation service**  
    - `createConversation()`  
    - `appendMessage(conversationId, sender, text)`  
    - `getConversationMessages(conversationId)`

- **Data access**
  - A compact Postgres client configured via `DATABASE_URL`.  
  - All SQL lives in one place, so switching DBs or adding migrations is straightforward.

This separation makes it easy to later plug in other channels (WhatsApp, Instagram, etc.) that reuse the same services.

***

## ğŸ§  8. LLM Integration

### 8.1. Provider
- âš™ï¸ **Provider**: OpenRouter  
- ğŸ”‘ **Auth**: `OPENROUTER_API_KEY` via environment variables.

### 8.2. Prompting strategy

Each LLM call includes:

- **System prompt** (simplified):

  > You are a helpful support agent for a small eâ€‘commerce store.  
  > Answer clearly and concisely.  
  > You know the storeâ€™s shipping, returns, refunds, and support hours.

- **Conversation history**:  
  - A sliding window of previous messages from the DB (to keep context and control tokens).  
- **User message**:  
  - The latest user input from the chat UI.

### 8.3. Domain knowledge / FAQs

The agent is aware of:

- ğŸ“¦ Shipping policy (regions, typical delivery times, basics of shipping cost).  
- ğŸ” Return/refund policy (window, condition, refund timing).  
- ğŸ•’ Support hours (time zone, weekday/weekend availability).

For this exercise, these are **hardâ€‘coded in the system prompt** rather than stored in the DB.

### 8.4. Guardrails & limits

- Rejects empty messages and trims very long messages.  
- Catches API errors (timeouts, invalid key, rate limits) and returns friendly messages instead of crashing.  
- Limits history length to keep token usage manageable.  
- Logs unexpected errors serverâ€‘side while returning safe responses to the client.

***

## ğŸ›¡ï¸ 9. Error Handling & Robustness

- âœ… **Input validation**
  - Backend checks `message` exists and is a string.  
  - Frontend disables sending when the input is empty or only whitespace.

- âœ… **Network/LLM errors**
  - Backend wraps calls in `try/catch` and returns JSON errors.  
  - Frontend displays an error banner instead of silently failing.

- âœ… **CORS & security**
  - CORS allows only:
    - `http://localhost:5173` (dev)  
    - `https://spur-ai-support-agent.vercel.app` (prod)  
  - API keys and DB credentials live only in environment variables.

***

## â˜ï¸ 10. Deployment

### 10.1. Backend â€“ Render

- Type: Web Service  
- Root directory: `backend`  
- Build command: `npm install`  
- Start command: `npm run start` (or equivalent)  

Environment variables:

- `PORT`  
- `DATABASE_URL`  
- `OPENROUTER_API_KEY` 
- `FRONTEND_URL=https://spur-ai-support-agent.vercel.app`  

Status page: `https://spur-ai-support-agent.onrender.com`.

### 10.2. Frontend â€“ Vercel

- Imported from the same GitHub repo.  
- Root directory: `frontend`  
- Framework preset: Svelte / Vite  

Environment variables:

- `VITE_API_BASE_URL=https://spur-ai-support-agent.onrender.com`  

Production URL: `https://spur-ai-support-agent.vercel.app`.

***

## âš–ï¸ 11. Tradeâ€‘offs & â€œIf I Had More Timeâ€¦â€

**Intentional simplifications:**

- Domain knowledge is promptâ€‘based, not a full knowledge base or RAG system.  
- No auth; conversations are anonymous and keyed by a random `sessionId`.  
- Single provider (OpenRouter), single model, no provider abstraction.

**Future improvements:**

- ğŸ“š Move FAQs into the DB and build prompts from structured data.  
- ğŸ”Œ Introduce a channel abstraction (web, WhatsApp, IG) all reusing the same conversation + LLM services.  
- âœ… Add unit and integration tests covering:
  - LLM service  
  - DB CRUD functions  
  - Chat endpoint contract  
- ğŸ“ˆ Add basic analytics: latency, token usage per conversation, success/error rates.

---

## âœ… 12. How to Evaluate Quickly

1. Visit `https://spur-ai-support-agent.vercel.app`.  
2. Ask questions like:
   - â€œWhat is your return policy?â€  
   - â€œDo you ship to the USA?â€  
   - â€œWhat are your support hours?â€  
3. Refresh the page â€“ the previous chat should reload (conversation history from Postgres).  
4. Open `https://spur-ai-support-agent.onrender.com` to see backend status and â€œOpen chat UIâ€ button.

---

## ğŸ§¾ Credits

Built with ğŸ§  code, â˜• coffee, and a lot of debugging.  
Made with â¤ï¸ by **Sunkara Purnasekhar**.
